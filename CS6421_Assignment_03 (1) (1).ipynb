{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lzwClNt3S3Tw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os, re, random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKObBRu3U7YU"
      },
      "source": [
        "# Data\n",
        "\n",
        "The dataset consists of a collection of dialogue from films. We will attempt to train a basic transformer-based chatbot, that can predict a response to a given block of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQaS_bh_U6vG",
        "outputId": "8bc072ea-984d-49be-8ad6-e9885d9de597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "\u001b[1m9916637/9916637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "dataset_download = tf.keras.utils.get_file(\n",
        "    'cornell_movie_dialogs.zip',\n",
        "    origin=\n",
        "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
        "    extract=True)\n",
        "data_path = os.path.join(\n",
        "    os.path.dirname(dataset_download), \"cornell_movie_dialogs_extracted/cornell movie-dialogs corpus\")\n",
        "\n",
        "lines_path = os.path.join(data_path, 'movie_lines.txt')\n",
        "conversations_path = os.path.join(data_path, 'movie_conversations.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZeGu8WYU9-l",
        "outputId": "2f5d70e9-443a-44ef-ad9b-42ccb8b29e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again . |\n",
            "> well , i thought we d start with pronunciation , if that s okay with you . |\n",
            "> not the hacking and gagging and spitting part . please . |\n",
            "answers: \n",
            "> well , i thought we d start with pronunciation , if that s okay with you . |\n",
            "> not the hacking and gagging and spitting part . please . |\n",
            "> okay . . . then how bout we try out some french cuisine . saturday ? night ? |\n"
          ]
        }
      ],
      "source": [
        "# limit the size of the dataset\n",
        "MAX_SAMPLES = 50000\n",
        "VOCAB_SIZE = 2**13\n",
        "MAX_LENGTH = 15\n",
        "# We will add special characters to indicate the start and end of a line\n",
        "START_TOKEN, END_TOKEN = \">\", \"|\"\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  # adding a start and an end token to the sentence\n",
        "  sentence =  START_TOKEN+' '+sentence+' '+END_TOKEN\n",
        "  return sentence\n",
        "\n",
        "def load_conversations():\n",
        "  # dictionary of line id to text\n",
        "  id2line = {}\n",
        "  with open(lines_path, errors='ignore') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    id2line[parts[0]] = preprocess_sentence(parts[4])\n",
        "\n",
        "  inputs, outputs = [], []\n",
        "  with open(conversations_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    # get conversation in a list of line ID\n",
        "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "    for i in range(len(conversation) - 1):\n",
        "      inputs.append(id2line[conversation[i]])\n",
        "      outputs.append(id2line[conversation[i + 1]])\n",
        "      if len(inputs) >= MAX_SAMPLES:\n",
        "        return inputs, outputs\n",
        "  return inputs, outputs\n",
        "\n",
        "\n",
        "questions, answers = load_conversations()\n",
        "for q in questions[:3]:\n",
        "    print(q)\n",
        "print(\"answers: \")\n",
        "for a in answers[:3]:\n",
        "    print(a)\n",
        "\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE, standardize='lower',ragged=False, output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "encoder.adapt(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OolW3QmuV9Cn",
        "outputId": "2fedf901-8360-4a9c-b855-4fcf5299f8d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample question: > you taught her , i suppose . . . |\n",
            "Sample answer: > that s right ! a girl s gotta know how to defend herself , don t she ? |\n",
            "Encoded question: [   4    6 1332   74    5    7  430    2    2    2    3    0    0    0\n",
            "    0]\n",
            "Encoded answer: [   4   15   12   71   19   11  197   12  244   31   55   10 3089 1389\n",
            "    5]\n"
          ]
        }
      ],
      "source": [
        "i = random.randint(0,len(questions)-1)\n",
        "print('Sample question: {}'.format(questions[i]))\n",
        "print('Sample answer: {}'.format(answers[i]))\n",
        "\n",
        "print('Encoded question: {}'.format(encoder(questions[i])))\n",
        "print('Encoded answer: {}'.format(encoder(answers[i])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBNyaxNrYFj4",
        "outputId": "2eaba41a-04cb-4f00-9177-6f4a1fff32cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 8192\n",
            "Number of samples: 50000\n"
          ]
        }
      ],
      "source": [
        "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
        "print('Number of samples: {}'.format(len(questions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ynEo9mYJhg"
      },
      "source": [
        "## Create a TF Dataset\n",
        "\n",
        "The transformer take two inputs:\n",
        "\n",
        "**inputs** - The prompt sentences from which our transformer tries to generate a response\n",
        "\n",
        "**dec-inputs** - At each time step, the transformer will predict a single word in response. dec-inputs tracks the words that the transformer has predicted so far as a response. At training time, this can be left as the response senteces we are training for.\n",
        "\n",
        "**outputs** - The output sentences we are training to predict\n",
        "\n",
        "\n",
        "We do have to be careful with the relative alignment of **dec-inputs**, and **outputs** - at each index, a value of **dec-inputs** should correspond to the word preceeding the corresponding word in **outputs**, e.g.\n",
        "\n",
        "For the input \"> What happend ? Where did you go ? |\" and response \"> I went to the shop . why ? |\" , we would have:\n",
        "\n",
        "1. **inputs** \"> What happend ? Where did you go ?\n",
        "2. **dec-inputs** \"> I went to the shop . why ?\"\n",
        "3. **outputs** \"I went to the shop . why ? |\"\n",
        "\n",
        "Observe how we achieve the desired alignment by dropping the > in dec-inputs, and the | in outputs\n",
        "\n",
        "Inputs and dec-inputs (or outputs) can have different lengths, but dec-inputs and outputs must be the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNVAQm8HYIA_",
        "outputId": "84531a2a-6edc-41e9-afe1-06f95c1e22b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[   4   42   23 ...   39  383   89]\n",
            " [   4   69    5 ...   15   12  126]\n",
            " [   4   37    9 ...    3    0    0]\n",
            " ...\n",
            " [   4  106   25 ...    0    0    0]\n",
            " [   4    7  247 ...   43 3988  573]\n",
            " [   4   86    5 ...  531    9  970]], shape=(50000, 15), dtype=int64)\n",
            "<_TensorSliceDataset element_spec=({'inputs': TensorSpec(shape=(15,), dtype=tf.int64, name=None), 'dec_inputs': TensorSpec(shape=(14,), dtype=tf.int64, name=None)}, {'outputs': TensorSpec(shape=(14,), dtype=tf.int64, name=None)})>\n",
            "({'inputs': <tf.Tensor: shape=(15,), dtype=int64, numpy=\n",
            "array([   4,   42,   23,  113,   25,  975,    8,    1,    1,   16, 8082,\n",
            "       3774,   39,  383,   89])>, 'dec_inputs': <tf.Tensor: shape=(14,), dtype=int64, numpy=\n",
            "array([  4,  69,   5,   7, 155,  23,  75, 295,  40,   1,   5,  53,  15,\n",
            "        12])>}, {'outputs': <tf.Tensor: shape=(14,), dtype=int64, numpy=\n",
            "array([ 69,   5,   7, 155,  23,  75, 295,  40,   1,   5,  53,  15,  12,\n",
            "       126])>})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=({'inputs': TensorSpec(shape=(None, 15), dtype=tf.int64, name=None), 'dec_inputs': TensorSpec(shape=(None, 14), dtype=tf.int64, name=None)}, {'outputs': TensorSpec(shape=(None, 14), dtype=tf.int64, name=None)})>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 20000\n",
        "encoded_qs = encoder(questions)\n",
        "encoded_as = encoder(answers)\n",
        "print(encoded_qs)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': encoded_qs,\n",
        "        'dec_inputs':encoded_as[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': encoded_as[:, 1:]\n",
        "    },\n",
        "))\n",
        "# printing the dataset to see how the tensor_slices function works\n",
        "print(dataset)\n",
        "for val in dataset:\n",
        "    print(val)\n",
        "    break\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmQd2pHBd97Q",
        "outputId": "5a7767eb-456f-4307-af57-20ee4475cb29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=({'inputs': TensorSpec(shape=(None, 15), dtype=tf.int64, name=None), 'dec_inputs': TensorSpec(shape=(None, 14), dtype=tf.int64, name=None)}, {'outputs': TensorSpec(shape=(None, 14), dtype=tf.int64, name=None)})>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6hmczXSeo14"
      },
      "source": [
        "# Defining a Transformer Model\n",
        "\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/transformer.png' width=40%/>\n",
        "\n",
        "\n",
        "## Positional Encoding\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png' width=40%/>\n",
        "\n",
        "Source: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
        "\n",
        "Since the attention layers will lose the sense of ordering of the inputs (i.e., [cat on mat] and [mat on cat] would be completely indistinguishable), we need a way to explicitly create some sort of encoding of the temporal positions of input elements.\n",
        "\n",
        "For example, consider a sentence \"cat on mat\", vectorized as \"[5 3 7]\"\n",
        "\n",
        "The next step was to create a vectorized embedding that is trained to represent the 'meaning' of particular words in a vector. We make sure to represent infomation about the *index (or position)* of each word in this vector, as well as its meaning.\n",
        "\n",
        "i.e., the embedding performs the mapping\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{c}\n",
        "5 \\\\\n",
        "3 \\\\\n",
        "7\n",
        "\\end{array}\\right),\n",
        "\\left(\\begin{array}{cc}\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right) → \\left(\\begin{array}{cccc}\n",
        "e_{00} & e_{01} & \\dots & e_{0d} \\\\\n",
        "e_{10} & e_{21} & \\dots & e_{1d}\\\\\n",
        "e_{20} & e_{21} & \\dots & e_{2d} \\\\\n",
        "\\end{array}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "Word embeddings such as Word2Vec convert the encoded word 5 (\"cat\") to a vector. We also need a method to convert the respective indices/positions (e.g. 0) to some useful vector format\n",
        "\n",
        "### Positional Encoding Formula\n",
        "The original authors of *Attention Is All You Need* proposed the following to embed indices as vectors.\n",
        "\n",
        "\\begin{align}\n",
        "  PE_{pos,2i}=&sin(\\frac{pos}{10000^{2i/d}}) \\\\\n",
        "  PE_{pos,2i+1}=&cos(\\frac{pos}{10000^{2i/d}}) \\\\\n",
        "\\end{align}\n",
        "\n",
        "Here\n",
        "\n",
        "a. $PE(pos,x)$ is the $x^{th}$ element of the outputted embedding of the index $pos$\n",
        "a. $pos$ is the index of the particular word being embedded \\\\\n",
        "b. $d$ is the chosen length of the word embedding vectors \\\\\n",
        "\n",
        "The main reasons why this particular method of method are usedul are:\n",
        "\n",
        "1. Every index is mapped to a unique vector\n",
        "2. Indices that are close in value are mapped to similar vectors\n",
        "3. The vectors are guaranteed to be within [-1,1]\n",
        "4. Its embeddings are very flexbile - they can be extended reliably to longer lengths than have been seen during training\n",
        "\n",
        "\n",
        "## Bringing it all together\n",
        "\n",
        "There are many strategies as to how to combine the embedding vector for the positions. Given word embedding as we saw last week - such as:\n",
        "\n",
        "<img src='https://www.tensorflow.org/static/text/guide/images/embedding2.png' width=40%/>\n",
        "\n",
        "We can simply add the position embedding vectors to their respect word embeddings. In practice, the weighted sum is often taken - e.g. for an embedding size of $d$ (4 in the example above), it is common to see\n",
        "\n",
        " `Overall Embedding = (word embedding)*W+(position embedding)`\n",
        "\n",
        "In order to increase or decrease the relative importance of temporal information in the model.\n",
        "\n",
        "### Worked example\n",
        "\n",
        "Given sentence: \"cat on mat\"\n",
        "\n",
        "Assume out vectorizer encodes this as: [5 3 7]\n",
        "\n",
        "And our word embedding (of size 4) generates the  embeddings for each word corresponding to the image:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{c}\n",
        "5 \\\\\n",
        "3 \\\\\n",
        "7\n",
        "\\end{array}\\right)→\n",
        "\\left(\\begin{array}{cccc}\n",
        "1.2 & -0.1 & 4.3 & 3.2 \\\\\n",
        "0.4 &2.5& -0.9& 0.5 \\\\\n",
        "2.1 & 0.3 & 0.1 & 0.4\n",
        "\\end{array}\\right),\n",
        "\\end{equation}\n",
        "\n",
        "We now need to apply the positional encoding to the vector of indices [0 1 2] to generate a position embedding of length 4\n",
        "\n",
        "#### Encoding index 0\n",
        "\n",
        "\\begin{align}\n",
        " PE(0) = [PE(0,0),PE(0,1),PE(0,2),PE(0,3)] \\\\\n",
        " PE(0,0) = sin(\\frac{0}{10000^{0/4}}) = 0 \\\\\n",
        " PE(0,1) = cos(\\frac{0}{10000^{0/4}}) = 1 \\\\\n",
        " PE(0,2) = sin(\\frac{0}{10000^{2/4}}) = 0 \\\\\n",
        " PE(0,3) = cos(\\frac{0}{10000^{2/4}}) = 1 \\\\\n",
        " \\implies PE(0)=[0 \\; 1\\; 0\\; 1]\n",
        "\\end{align}\n",
        "\n",
        "#### Encoding index 1\n",
        "\\begin{align}\n",
        " PE(1) = [PE(1,0),PE(1,1),PE(1,2),PE(1,3)] \\\\\n",
        " PE(1,0) = sin(\\frac{1}{10000^{0/4}}) = 0.84 \\\\\n",
        " PE(1,1) = cos(\\frac{1}{10000^{0/4}}) = 0.54 \\\\\n",
        " PE(1,2) = sin(\\frac{1}{10000^{2/4}}) = 0.0001 \\\\\n",
        " PE(1,3) = cos(\\frac{1}{10000^{2/4}}) = 1.0 \\\\\n",
        " \\implies PE(1)=[0.84\\; 0.54\\; 0.0001\\; 1.0]\n",
        "\\end{align}\n",
        "\n",
        "#### Encoding index 2\n",
        "\n",
        "\\begin{align}\n",
        " PE(2) = [PE(2,0),PE(2,1),PE(2,2),PE(2,3)] \\\\\n",
        " PE(2,0) = sin(\\frac{2}{10000^{0/4}}) = 0.91 \\\\\n",
        " PE(2,1) = cos(\\frac{2}{10000^{0/4}}) = -0.42 \\\\\n",
        " PE(2,2) = sin(\\frac{2}{10000^{2/4}}) = 0.02\\\\\n",
        " PE(2,3) = cos(\\frac{2}{10000^{2/4}}) = 1.0 \\\\\n",
        " \\implies PE(2)=[0.91 \\;0.42\\; 0.02\\; 1.0]\n",
        "\\end{align}\n",
        "\n",
        "#### Resulting in a positional embedding of:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{c}\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right)→\n",
        "\\left(\\begin{array}{cccc}\n",
        "1.0& 0.0 &1.0 &0.0\\\\\n",
        "0.54& 0.01& 1.0  &0.0001 \\\\\n",
        "-0.42& 0.02 &1.0 & 0.0002\n",
        "\\end{array}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "### Final result:\n",
        "Assuming  a weighting of 1, we get a final embedding of\n",
        "\\begin{equation}\n",
        "\\left(\\begin{array}{cccc}\n",
        "1.2 & -0.1 & 4.3 & 3.2 \\\\\n",
        "0.4 &2.5& -0.9& 0.5 \\\\\n",
        "2.1 & 0.3 & 0.1 & 0.4\n",
        "\\end{array}\\right)*1+\\left(\\begin{array}{cccc}\n",
        "0 & 1& 0& 1\\\\\n",
        "0.84& 0.54& 0.0001& 1.0 \\\\\n",
        "0.91 &-0.42& 0.02& 1.0\n",
        "\\end{array}\\right)=\\left(\\begin{array}{cccc}\n",
        "1.2 & 0.9 & 4.3 & 4.2 \\\\\n",
        "1.24 &3.04& -0.8999 & 1.5 \\\\\n",
        "3.01 & -0.12 &0.12 & 1.4\n",
        "\\end{array}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "10-iLTkhfVCF"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(input_length, embedding_size):\n",
        "\n",
        "  '''\n",
        "  Generate a dimension  input_length x embedding_size  matrix of positional encodings\n",
        "\n",
        "  Each row should correspond to a vector of length 'embedding_size' - the positional\n",
        "        encoding of the index corresponding to the row index\n",
        "  '''\n",
        "  positions = tf.reshape(tf.range(0,input_length,1,dtype=tf.float32), (-1,1))\n",
        "  div_factor =tf.constant(10_000, dtype=tf.float32)\n",
        "\n",
        "  i_pow = tf.realdiv( tf.range(0,embedding_size,2,dtype=tf.float32), tf.constant(embedding_size, dtype=tf.float32))\n",
        "  denoms =  tf.pow(div_factor, i_pow)\n",
        "  frac = tf.realdiv(positions,denoms)\n",
        "\n",
        "  odd_embeddings = tf.cos(frac)\n",
        "  even_embeddings = tf.sin(frac)\n",
        "\n",
        "  pos_encoding = tf.reshape(tf.stack([even_embeddings, odd_embeddings], axis=2), (input_length,-1))[:,:embedding_size]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5WXXFQzrt_p0"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, embedding_size):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    ''' Create an Embedding layer, to produce embeddings of\n",
        "        length embedding_size, with masking and the given vocab_size '''\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=True)\n",
        "\n",
        "    # Pre-compute a very large positional encoding matrix, and later\n",
        "    # we will take sub-sections that we need out of it\n",
        "    self.pos_encoding = positional_encoding(2048, embedding_size)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.embedding_size, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXE1d8B8X5J2"
      },
      "source": [
        "## Scaled Dot Product Attention\n",
        "\n",
        "### Principles of the Attention function\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png' width=50%/>\n",
        "\n",
        "There are two inputs:\n",
        "\n",
        "1. The query sequence; the sequence being processed; the sequence doing the attending (bottom).\n",
        "2. The context sequence; the sequence being attended to (left).\n",
        "The output has the same shape as the query-sequence.\n",
        "\n",
        "The common comparison is that this operation is like a dictionary lookup. A fuzzy, differentiable, vectorized dictionary lookup.\n",
        "\n",
        "Here's a regular python dictionary, with 3 keys and 3 values being passed a single query:\n",
        "\n",
        "```python3\n",
        "d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n",
        "result = d['color']\n",
        "```\n",
        "\n",
        "1. The query is what you're trying to find.\n",
        "2. The key is what sort of information the dictionary has.\n",
        "3. The value is that information.\n",
        "\n",
        "When you look up a query in a regular dictionary, the dictionary finds the matching key, and returns its associated value. The query either has a matching key or it doesn't. You can imagine a fuzzy dictionary where the keys don't have to match perfectly. If you looked up d[\"species\"] in the dictionary above, maybe you'd want it to return \"pickup\" since that's the best match for the query.\n",
        "\n",
        "An attention layer does a fuzzy lookup like this, but it's not just looking for the best key. It combines the values based on how well the query matches each key.\n",
        "\n",
        "How does that work? In an attention layer the query, key, and value are each vectors. Instead of doing a hash lookup the attention layer combines the query and key vectors to determine how well they match, the \"attention score\". The layer returns the average across all the values, weighted by the \"attention scores\".\n",
        "\n",
        "\n",
        "### Scaled dot product Attention function\n",
        "\n",
        "The scaled dot-product attention function used to implement attention by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
        "\n",
        "\n",
        "$${Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
        "\n",
        "As the softmax normalization is done on the `key`, its values decide the amount of importance given to the `query`.\n",
        "\n",
        "The output represents the multiplication of the attention weights and the `value` vector. This ensures that the words we want to focus on are kept as is and the irrelevant words are flushed out.\n",
        "\n",
        "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.\n",
        "\n",
        "For example, consider that `query` and `key` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `query` and `key` should have a mean of 0 and variance of 1, so that we get a gentler softmax.\n",
        "\n",
        "#### Masking\n",
        "\n",
        "All of our vectorized sentences are a fixed length - this was accomplished by either tuncating sentences that were too long, or by padding shorter sentences with 0 values.\n",
        "\n",
        "In the case of these 0 padded sequences, we want to make sure that not of the 0 padding values receive any significant weighting by the Attention layer.\n",
        "\n",
        "To accomplish this, we will keep track of a 'mask' - a tensor of the same shape as the input that indicates with True/False values whether the value at the corresponding index of the input is a real (non-dummy) value or not.\n",
        "\n",
        "For values of $Q$ corresponding to False mask values, we make sure to set $Attention(Q, K, V)=softmax_k(-\\infty)=0$\n",
        "\n",
        "## Global Self-attention\n",
        "\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new-full.png' width=45%/>\n",
        "\n",
        "In the case of Global Self Attention, as is used in the Transormer's Encoder, the inputted sequence is used as both the Query and Key/Value.\n",
        "\n",
        "\n",
        "A point to note - the embedding size is sometimes referred to as the model \"depth\". We will use the term \"depth\" (denoted $d_k$, `depth` or `d_model`) when talking about the embedding widths of operations within the transformer, and \"embedding width\" in the context of word vectorization and positional embedding layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R_l4bBKszi_U"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WFkrfzI2q_R1"
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, queries, keys, values, mask=None):\n",
        "        '''\n",
        "        SCALED DOT-PRODUCT ATTENTION BETWEEN queries keys, values\n",
        "        '''\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        numerator = tf.matmul(queries, keys, transpose_b=True)\n",
        "        embedding_dim = tf.math.sqrt(tf.cast(tf.shape(keys)[-1], tf.float32))\n",
        "        normalized_values = tf.realdiv(numerator, embedding_dim)\n",
        "        if mask is not None:\n",
        "            normalized_values += (1.0 - mask) * -1e9\n",
        "        attention_weights = tf.nn.softmax(normalized_values, axis=-1)\n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return tf.matmul(attention_weights, values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wApRGMy2fQrB"
      },
      "source": [
        "## Multi-head attention\n",
        "\n",
        "<img src='https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png' wdith=40%/>\n",
        "\n",
        "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.\n",
        "\n",
        "The scaled_dot_product_attention defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step. The attention output for each head is then concatenated (using tf.transpose, and tf.reshape) and put through a final Dense layer.\n",
        "\n",
        "Instead of one single attention head, query, key, and value are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TubsMRaffcqK"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dp_attn = DotProductAttention()\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # scaled dot-product attention\n",
        "    scaled_attention = self.dp_attn(queries=query, keys=key, values=value,\n",
        "                                    mask=mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # concatenation of heads\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # final linear layer\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Bq4x4Xm74d"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png' width=60%/>\n",
        "\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "1. Multi-head attention (with padding mask)\n",
        "2. 2 dense layers followed by dropout\n",
        "\n",
        "Each of these sublayers has a residual connection (by adding the input of the sublayer to its output) around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o3liCmwxng2Y"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "\n",
        "    self.mh_attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")\n",
        "\n",
        "    self.lnorm1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.lnorm2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x,mask=inputs\n",
        "    #x, mask = inputs['x'], inputs['mask']\n",
        "    attention_output = self.mh_attention({\n",
        "          'query': x,\n",
        "          'key': x,\n",
        "          'value': x,\n",
        "          'mask': mask\n",
        "      })\n",
        "    ''' Add & Norm\n",
        "    1. Add x to the attention output, and apply norm\n",
        "    2. Add the output of the dense layers to the attention output, and apply norm\n",
        "    '''\n",
        "    '''\n",
        "      HERE\n",
        "    '''\n",
        "    sub_layer_1_op = self.lnorm1(tf.add(x, attention_output))\n",
        "    feed_forward_op = self.seq(sub_layer_1_op)\n",
        "    normalised_output2 = self.lnorm2(tf.add(sub_layer_1_op, feed_forward_op))\n",
        "    return normalised_output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tiBliItdkwUI"
      },
      "outputs": [],
      "source": [
        "sample_encoder_layer = EncoderLayer(\n",
        "    dff=512,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    dropout_rate=0.3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpO-TwYjnuCr"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "The Encoder consists of:\n",
        "\n",
        "1. Input Embedding\n",
        "2. Positional Encoding\n",
        "3. num_layers encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rKWq25pSpjUP"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, embedding_size=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    mask = create_padding_mask(x)\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i]([x,mask])\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s2_442fkp40Q"
      },
      "outputs": [],
      "source": [
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=len(encoder.get_vocabulary()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN53-Wqe1m1Y"
      },
      "source": [
        "## Decoder Layer\n",
        "\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png' width=50%/>\n",
        "\n",
        "The Decoder has some slightly different attention mechanisms compared to the Encode, **mainly Causal Self Attention**. This consists of the use careful masking to ensure that when we are using the same series for key, value and query - we only allow the model to apply the attention for a given value of Q, to values of K,V that have occur **before** Q.  \n",
        "\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png' width=30%/>\n",
        "\n",
        "The structure of a decoder layer is illustrated above, consisting of:\n",
        "\n",
        "1.  Multi head attention with a causal mask, followed by a residual connection and layer normalization. The attention is applied with the **Embedded outputs** as all of the Key/Value and Query values.\n",
        "2. The output of these operations are passed to another MHA as the Query, with the Key/Value being the Encoder's output. A Residual connection and noprmalization are again applied.\n",
        "3. The output of this second MHA goes to a pair of feedforward layers with Layer norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "meOJH11Q42Cw"
      },
      "outputs": [],
      "source": [
        "def create_causal_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  causal_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(causal_mask, padding_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6SdXHIqe1zlW"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "\n",
        "\n",
        "    self.attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads)\n",
        "    self.attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads)\n",
        "\n",
        "\n",
        "    self.lnorm1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.lnorm2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.lnorm3=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropoutLayer=tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    y, la_mask, e_out, p_mask = inputs['y'], inputs['look_ahead_mask'],inputs['enc_outputs'],inputs['padding_mask']\n",
        "    att1 = self.attention1({\n",
        "        'query' :y,\n",
        "        'key':y,\n",
        "        'value':y,\n",
        "        'mask':la_mask\n",
        "    })\n",
        "    postatt1 = tf.add(att1, y)\n",
        "    postatt1 = self.lnorm1(postatt1)\n",
        "\n",
        "    att2 = self.attention2({\n",
        "        'query' : postatt1,\n",
        "        'key':e_out,\n",
        "        'value':e_out,\n",
        "        'mask': p_mask\n",
        "    })\n",
        "    '''\n",
        "    Add & Norm\n",
        "    1. Apply the droppout to att2\n",
        "    1. Add att1, then apply norm\n",
        "    '''\n",
        "    att2 = self.dropoutLayer(att2)\n",
        "    comb_att = self.lnorm2(tf.add(att2, postatt1))\n",
        "\n",
        "    '''\n",
        "    OUTPUT - Apply self.seq, then norm\n",
        "    '''\n",
        "\n",
        "    ff_out = self.seq(comb_att)\n",
        "    outputs = self.lnorm3(tf.add(ff_out, comb_att))\n",
        "    return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bv_Z_v2675xa"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, embedding_size=d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    y, encoded, la_mask, dp_mask = inputs['dec_inputs'],inputs['enc_outputs'],inputs['look_ahead_mask'], inputs['dec_padding_mask']\n",
        "\n",
        "    '''Apply positional embedding'''\n",
        "    y = self.pos_embedding(y)\n",
        "    # calling the dropout here as well like in the encoder method\n",
        "    y = self.dropout(y)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      y = self.dec_layers[i]({'y':y, 'enc_outputs' : encoded, 'look_ahead_mask':la_mask, 'padding_mask':dp_mask})\n",
        "\n",
        "    return y  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8vuXj9A8oRr",
        "outputId": "ff0b8a79-4285-4186-e12a-ffa4200def5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'encoder_layer_12' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sample_encoder = Encoder(\n",
        "    vocab_size=8192,\n",
        "    num_layers=2,\n",
        "    dff=128,\n",
        "    d_model=128,\n",
        "    num_heads=2,\n",
        "    dropout_rate=0.3)\n",
        "sample_encoder_output = sample_encoder(encoded_qs[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyWLbLiJcSOt",
        "outputId": "9566036e-7ee7-4287-9b46-cca328e46d4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.6114156 , -0.16786902,  1.1411777 , ..., -0.5097391 ,\n",
              "          0.12941796, -0.43437693],\n",
              "        [ 0.8403258 , -0.42567185,  1.471851  , ..., -0.9844093 ,\n",
              "          0.15804482, -0.6434351 ],\n",
              "        [ 0.2787432 , -0.40695375,  1.4411424 , ..., -1.3865196 ,\n",
              "         -0.01284257, -0.5562935 ],\n",
              "        ...,\n",
              "        [ 0.19258921, -0.02948193, -0.35875225, ..., -1.2588646 ,\n",
              "          0.560844  ,  0.20794635],\n",
              "        [ 0.1260828 , -0.52209073, -0.18675578, ..., -1.0425204 ,\n",
              "          0.2810393 , -0.60065085],\n",
              "        [ 1.0788563 , -0.97025293,  0.39189124, ..., -1.3838252 ,\n",
              "          0.18631837, -0.68183243]],\n",
              "\n",
              "       [[-0.14550424, -0.16351724,  1.1713383 , ..., -0.78990483,\n",
              "          0.15848486, -0.22528379],\n",
              "        [ 0.8546194 , -0.21870963,  1.0142318 , ..., -0.80236334,\n",
              "          0.24037679, -0.08388297],\n",
              "        [ 0.8754331 , -0.8380729 ,  1.8214546 , ..., -0.64354974,\n",
              "         -0.15047568, -0.46243635],\n",
              "        ...,\n",
              "        [ 0.21323715,  0.28404266, -0.31144547, ..., -0.81725454,\n",
              "         -0.05502677, -0.11773516],\n",
              "        [ 0.26144844, -0.30105907, -0.18373364, ..., -1.1897962 ,\n",
              "          0.15687543, -0.50290984],\n",
              "        [ 0.16233875, -0.32799417,  0.31916034, ..., -0.7552938 ,\n",
              "          0.04377967, -0.23945017]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "sample_encoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmcWUiWnWaep",
        "outputId": "b86cc26b-0c94-4afe-c6a0-a82a3ea24075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'encoder_layer_14' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'decoder_layer_2' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.8736555  -1.0092758   2.5874782  ... -0.67956865 -0.00750333\n",
            "   -0.536631  ]\n",
            "  [-0.37509516 -0.85679704  3.1082504  ... -0.4527932  -0.18317868\n",
            "   -0.909125  ]\n",
            "  [-0.3119005  -1.1165025   2.885539   ... -0.6697498  -0.14475743\n",
            "   -0.7262883 ]\n",
            "  ...\n",
            "  [-0.7708615  -0.6763275   2.8435743  ... -0.9458972  -0.40153852\n",
            "   -0.557855  ]\n",
            "  [-1.0510508  -0.45387366  2.4623225  ... -1.0130867  -0.15757123\n",
            "   -0.5792247 ]\n",
            "  [-0.37994328 -0.5446398   2.273072   ... -0.4884858  -0.35871547\n",
            "   -0.619769  ]]\n",
            "\n",
            " [[-0.90132266 -1.133315    2.6474166  ... -0.663186    0.20833561\n",
            "   -0.54096663]\n",
            "  [-0.6630191  -0.91916585  2.9922233  ... -0.6359427  -0.05261474\n",
            "   -0.6961169 ]\n",
            "  [-0.58826935 -1.314211    3.081327   ... -0.51861477  0.22472447\n",
            "   -0.7598954 ]\n",
            "  ...\n",
            "  [-0.49256623 -0.8288111   2.5765746  ... -1.073414   -0.13097905\n",
            "   -0.5988776 ]\n",
            "  [-0.38516927 -1.026417    2.2659578  ... -0.9539077  -0.1621902\n",
            "   -0.78557473]\n",
            "  [-0.3298869  -0.8745144   2.1818695  ... -1.1124492  -0.17225996\n",
            "   -0.6452331 ]]], shape=(2, 14, 128), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "x,y=encoded_qs[0:2],encoded_as[0:2,:-1]\n",
        "sample_encoder = Encoder(\n",
        "    vocab_size=8192,\n",
        "    num_layers=2,\n",
        "    dff=512,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    dropout_rate=0.3)\n",
        "sample_encoder_output = sample_encoder(x)\n",
        "\n",
        "sample_decoder = Decoder(vocab_size=8192,\n",
        "    num_layers=2,\n",
        "    dff=512,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    dropout_rate=0.3)\n",
        "\n",
        "output = sample_decoder(\n",
        "    {'dec_inputs':y,\n",
        "     'enc_outputs':sample_encoder_output,\n",
        "     'look_ahead_mask': create_causal_mask(y),\n",
        "     'dec_padding_mask':create_padding_mask(x)\n",
        "     }\n",
        ")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1VeR1-C-4-6"
      },
      "source": [
        "## Transformer\n",
        "<img src='https://www.tensorflow.org/images/tutorials/transformer/transformer.png' />\n",
        "\n",
        "Finally, the transformer:\n",
        "\n",
        "1. Takes the inputted sentences and performs the embeddings\n",
        "2. Creates the encoder and decoder\n",
        "3. Creates final linear and softmax layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "r3aiUEIt-6lc"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        vocab_size=input_vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        dff=dff,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        vocab_size=input_vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        dff=dff,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    x, y  = inputs\n",
        "\n",
        "    context = self.encoder(x)  # (batch_size, context_len, d_model)\n",
        "    look_ahead_mask  =  create_causal_mask(y)\n",
        "    dec_padding_mask = create_padding_mask(x)\n",
        "    decoded = self.decoder({'dec_inputs':y, 'look_ahead_mask':look_ahead_mask, 'dec_padding_mask':dec_padding_mask, 'enc_outputs':context})  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(decoded)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwy6TV1DBD86"
      },
      "source": [
        "# Training - No Marks Awarded, this is for your own learning and information\n",
        "\n",
        "The original transformer paper proposed a custom learning rate scheduler. The details of this are outside the scope of the assignment, but an example is shown below for what the learning rate looks like under a embedding width of 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ltv5N4lSBFVQ"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qHXFdoBZIrM-"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(256)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "LKek2H6HIrqk",
        "outputId": "f847cc5b-aefc-447a-db87-ba95737cfbb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYOcDrLrMv2y"
      },
      "source": [
        "The loss an accuracy functions must also be custom defined to work with masking and other particulars of this transformer's input/output design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2AU2ecA-BH2F"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=False, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES7gYGyiBLO5",
        "outputId": "c166fd26-4839-4067-d02a-a7cb6198cc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'encoder_layer_16' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'decoder_layer_4' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "num_layers = 8\n",
        "d_model = 128\n",
        "dff = 32\n",
        "num_heads = 8\n",
        "dropout_rate = 0.15\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=len(encoder.get_vocabulary()),\n",
        "    target_vocab_size=len(encoder.get_vocabulary()),\n",
        "    dropout_rate=dropout_rate, name=\"outputs\")\n",
        "\n",
        "inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "\n",
        "outputs = transformer([inputs,dec_inputs])\n",
        "\n",
        "\n",
        "model = tf.keras.models.Model(inputs={'inputs':inputs,'dec_inputs':dec_inputs}, outputs={'outputs':outputs})\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "9r7mIQedEwpG",
        "outputId": "bf903478-97dd-4ef4-a4c9-c7f7bfe1d85b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_28\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_28\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ inputs (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_inputs (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ outputs (\u001b[38;5;33mTransformer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)     │      \u001b[38;5;34m4,882,944\u001b[0m │ inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ dec_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,882,944</span> │ inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ dec_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,882,944\u001b[0m (18.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,882,944</span> (18.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,882,944\u001b[0m (18.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,882,944</span> (18.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-8uSCWHQmHo",
        "outputId": "bc1e7e57-7582-4b11-cbb7-b868e401bfaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'outputs': <tf.Tensor: shape=(2, 1, 8192), dtype=float32, numpy=\n",
              " array([[[1.4838685e-04, 1.3290663e-04, 1.7989881e-04, ...,\n",
              "          8.2328908e-05, 1.1958829e-04, 1.0774903e-04]],\n",
              " \n",
              "        [[1.4530445e-04, 1.3164953e-04, 1.7812176e-04, ...,\n",
              "          7.9417921e-05, 1.1753648e-04, 1.0351325e-04]]], dtype=float32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "model({'inputs':encoded_qs[0:2],'dec_inputs':encoded_as[0:2,:1]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJZVrujLBNrq",
        "outputId": "ca6a727b-6316-4b05-b19a-61803d1e3682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 66ms/step - accuracy: 0.6830 - loss: 0.1120\n",
            "Epoch 2/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6843 - loss: 0.1138\n",
            "Epoch 3/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6830 - loss: 0.1135\n",
            "Epoch 4/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6836 - loss: 0.1128\n",
            "Epoch 5/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6844 - loss: 0.1151\n",
            "Epoch 6/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6822 - loss: 0.1157\n",
            "Epoch 7/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 59ms/step - accuracy: 0.6817 - loss: 0.1162\n",
            "Epoch 8/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6814 - loss: 0.1191\n",
            "Epoch 9/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6820 - loss: 0.1089\n",
            "Epoch 10/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6842 - loss: 0.1130\n",
            "Epoch 11/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6835 - loss: 0.1150\n",
            "Epoch 12/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 58ms/step - accuracy: 0.6845 - loss: 0.1128\n",
            "Epoch 13/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6834 - loss: 0.1167\n",
            "Epoch 14/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6817 - loss: 0.1101\n",
            "Epoch 15/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6839 - loss: 0.1106\n",
            "Epoch 16/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 62ms/step - accuracy: 0.6834 - loss: 0.1126\n",
            "Epoch 17/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6843 - loss: 0.1144\n",
            "Epoch 18/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.6847 - loss: 0.1071\n",
            "Epoch 19/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6823 - loss: 0.1175\n",
            "Epoch 20/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6826 - loss: 0.1170\n",
            "Epoch 21/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6842 - loss: 0.1129\n",
            "Epoch 22/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6840 - loss: 0.1057\n",
            "Epoch 23/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6847 - loss: 0.1032\n",
            "Epoch 24/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6828 - loss: 0.1036\n",
            "Epoch 25/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6851 - loss: 0.1044\n",
            "Epoch 26/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6838 - loss: 0.1033\n",
            "Epoch 27/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6848 - loss: 0.1077\n",
            "Epoch 28/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6848 - loss: 0.1039\n",
            "Epoch 29/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6833 - loss: 0.1014\n",
            "Epoch 30/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6849 - loss: 0.1026\n",
            "Epoch 31/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6853 - loss: 0.0995\n",
            "Epoch 32/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6849 - loss: 0.1030\n",
            "Epoch 33/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6845 - loss: 0.1032\n",
            "Epoch 34/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6837 - loss: 0.1037\n",
            "Epoch 35/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step - accuracy: 0.6868 - loss: 0.1045\n",
            "Epoch 36/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6847 - loss: 0.1005\n",
            "Epoch 37/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6858 - loss: 0.1011\n",
            "Epoch 38/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6829 - loss: 0.1001\n",
            "Epoch 39/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6844 - loss: 0.0982\n",
            "Epoch 40/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6854 - loss: 0.1003\n",
            "Epoch 41/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6858 - loss: 0.0964\n",
            "Epoch 42/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6842 - loss: 0.1025\n",
            "Epoch 43/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6859 - loss: 0.0987\n",
            "Epoch 44/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6844 - loss: 0.1009\n",
            "Epoch 45/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6853 - loss: 0.0967\n",
            "Epoch 46/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step - accuracy: 0.6858 - loss: 0.0946\n",
            "Epoch 47/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6850 - loss: 0.0991\n",
            "Epoch 48/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6869 - loss: 0.0954\n",
            "Epoch 49/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6856 - loss: 0.0991\n",
            "Epoch 50/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6844 - loss: 0.1009\n",
            "Epoch 51/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6869 - loss: 0.0956\n",
            "Epoch 52/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6850 - loss: 0.0941\n",
            "Epoch 53/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6840 - loss: 0.0966\n",
            "Epoch 54/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6837 - loss: 0.0946\n",
            "Epoch 55/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6874 - loss: 0.0923\n",
            "Epoch 56/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6852 - loss: 0.0934\n",
            "Epoch 57/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6856 - loss: 0.0923\n",
            "Epoch 58/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6879 - loss: 0.0925\n",
            "Epoch 59/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6870 - loss: 0.0942\n",
            "Epoch 60/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6869 - loss: 0.0922\n",
            "Epoch 61/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6866 - loss: 0.0970\n",
            "Epoch 62/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6875 - loss: 0.0939\n",
            "Epoch 63/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.6850 - loss: 0.0936\n",
            "Epoch 64/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.6883 - loss: 0.0966\n",
            "Epoch 65/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.6858 - loss: 0.0913\n",
            "Epoch 66/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.6859 - loss: 0.0908\n",
            "Epoch 67/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6862 - loss: 0.0928\n",
            "Epoch 68/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6864 - loss: 0.0883\n",
            "Epoch 69/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 59ms/step - accuracy: 0.6862 - loss: 0.0897\n",
            "Epoch 70/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.6875 - loss: 0.0981\n",
            "Epoch 71/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6862 - loss: 0.0900\n",
            "Epoch 72/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 61ms/step - accuracy: 0.6888 - loss: 0.0888\n",
            "Epoch 73/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6867 - loss: 0.0906\n",
            "Epoch 74/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6870 - loss: 0.0921\n",
            "Epoch 75/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6884 - loss: 0.0904\n",
            "Epoch 76/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.6877 - loss: 0.0887\n",
            "Epoch 77/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6880 - loss: 0.0956\n",
            "Epoch 78/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.6866 - loss: 0.0870\n",
            "Epoch 79/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6874 - loss: 0.0902\n",
            "Epoch 80/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6901 - loss: 0.0864\n",
            "Epoch 81/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 61ms/step - accuracy: 0.6874 - loss: 0.0853\n",
            "Epoch 82/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6867 - loss: 0.0851\n",
            "Epoch 83/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.6877 - loss: 0.0891\n",
            "Epoch 84/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6860 - loss: 0.0861\n",
            "Epoch 85/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6820 - loss: 0.1101\n",
            "Epoch 86/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6878 - loss: 0.0877\n",
            "Epoch 87/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.6857 - loss: 0.0869\n",
            "Epoch 88/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6862 - loss: 0.0838\n",
            "Epoch 89/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.6882 - loss: 0.0846\n",
            "Epoch 90/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6880 - loss: 0.0847\n",
            "Epoch 91/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6871 - loss: 0.0854\n",
            "Epoch 92/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.6878 - loss: 0.0865\n",
            "Epoch 93/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6879 - loss: 0.0823\n",
            "Epoch 94/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6875 - loss: 0.0875\n",
            "Epoch 95/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.6876 - loss: 0.0858\n",
            "Epoch 96/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 61ms/step - accuracy: 0.6894 - loss: 0.0843\n",
            "Epoch 97/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6883 - loss: 0.0840\n",
            "Epoch 98/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 61ms/step - accuracy: 0.6887 - loss: 0.0829\n",
            "Epoch 99/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 62ms/step - accuracy: 0.6902 - loss: 0.0830\n",
            "Epoch 100/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 62ms/step - accuracy: 0.6871 - loss: 0.0848\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b18407d81d0>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "model.fit(dataset, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EweyCSOURLPP"
      },
      "source": [
        "# Evaluate Some Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ziWNg6g-RKRo"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  sentence = tf.expand_dims(encoder(sentence),0)\n",
        "\n",
        "  output = tf.expand_dims([encoder.get_vocabulary().index('>')],0)\n",
        "  for i in range(MAX_LENGTH):\n",
        "    predictions = model({'inputs':sentence, 'dec_inputs':output}, training=False)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions['outputs'][:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if tf.equal(predicted_id, encoder.get_vocabulary().index('|')):\n",
        "      break\n",
        "\n",
        "    # concatenated the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "  prediction = evaluate(sentence)\n",
        "  predicted_sentence = ''\n",
        "  for word in prediction:\n",
        "    predicted_sentence+=' '+encoder.get_vocabulary()[word]\n",
        "  #predicted_sentence = encoder.decode(\n",
        "  #    [i for i in prediction if i < encoder.vocab_size])\n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Output: {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "mb02KokYSlGT",
        "outputId": "42491fd5-8526-4c30-917c-ad46ab1f22db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Where have you been?\n",
            "Output:  > things .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' > things .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "predict('Where have you been?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "hJvMzdjwRQtj",
        "outputId": "8867dbaa-9413-4f94-9d2a-eda15acceb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: What was the thing that happened to him?\n",
            "Output:  > things .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' > things .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "predict('What was the thing that happened to him?')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JMYQM241GS0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs6421_env",
      "language": "python",
      "name": "cs6421_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}